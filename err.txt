ğŸš€  [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 2ê±´
ğŸš€  [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 14ê±´
ğŸš€  [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 25ê±´

âœ…  ì—…ë°ì´íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: output/cloud_updates_week_2025-05-07.json
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 1): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials a1e0cc1a2514c-8780b01fb8bsm1795966241.15 - gsmtp')
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 2): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials ada2fe7eead31-4daf208a3c7sm2104275137.0 - gsmtp')
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 3): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials a1e0cc1a2514c-8783eb34924sm334758241.33 - gsmtp')
â—ï¸  ëª¨ë“  ë©”ì¼ ë°œì†¡ ì¬ì‹œë„ ì‹¤íŒ¨
ğŸš€  [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 5ê±´
ğŸš€  [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 20ê±´
ğŸš€  [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘
ğŸ“„  [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - 36ê±´

âœ…  ì—…ë°ì´íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: output/cloud_updates_week_2025-05-08.json
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 1): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials a1e0cc1a2514c-8780b02018fsm2192972241.18 - gsmtp')
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 2): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials 71dfb90a1353d-52ae41f2065sm2665665e0c.47 - gsmtp')
âŒ  ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ 3): (535, b'5.7.8 Username and Password not accepted. For more information, go to\n5.7.8  https://support.google.com/mail/?p=BadCredentials ada2fe7eead31-4daf26722ffsm2508400137.28 - gsmtp')
â—ï¸  ëª¨ë“  ë©”ì¼ ë°œì†¡ ì¬ì‹œë„ ì‹¤íŒ¨
================
mythe82@instance-20250429-050226:~/csp-scrapper-uv/app$ cat main.py 
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
import json
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from sumy.summarizers.text_rank import TextRankSummarizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
import feedparser
from dotenv import load_dotenv
import os
import re
from deep_translator import GoogleTranslator
import matplotlib.pyplot as plt
import base64
from io import BytesIO
import numpy as np
import platform

# ---------- í™˜ê²½ë³€ìˆ˜ ë¡œë”© ----------
load_dotenv()

SMTP_SERVER = os.getenv("SMTP_SERVER")
SMTP_PORT = int(os.getenv("SMTP_PORT"))
SENDER = os.getenv("SENDER")
SENDER_PASSWORD = os.getenv("EMAIL_PASSWORD")
RECEIVERS = os.getenv("RECEIVERS").split(",")

# ---------- ë‚ ì§œ ë²”ìœ„ ----------
today = datetime.today()
start_of_week = today - timedelta(days=today.weekday())
end_of_week = start_of_week + timedelta(days=6)

all_data = []
stop_scraping = False

# ---------- ê³µí†µ í•¨ìˆ˜ ----------
def summarize_text(text, sentence_count=2, max_length=300):
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = TextRankSummarizer()
        summary = summarizer(parser.document, sentence_count)
        summary_text = " ".join(str(sentence) for sentence in summary)
        summary_text = summary_text.strip()

        if len(summary_text) > max_length:
            summary_text = summary_text[:max_length].rstrip() + "..."

        return summary_text
    except Exception:
        return text


def summarize_long_text(text, max_chunks=5, max_length=150):
    try:
        chunks = re.split(r'\n|\.\s+', text)
        summaries = []
        for chunk in chunks:
            chunk = chunk.strip()
            if len(chunk) < 50:
                continue
            summary = summarize_text(chunk, sentence_count=1, max_length=max_length)
            summaries.append(summary)
            if len(summaries) >= max_chunks:
                break
        return " ".join(summaries)
    except Exception as e:
        print(f"âŒ ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½ ì˜¤ë¥˜: {e}")
        return text


def translate_to_korean(text):
    try:
        return GoogleTranslator(source='en', target='ko').translate(text)
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {e}")
        return text


def clean_aws_summary(summary_html):
    try:
        summary_text = BeautifulSoup(summary_html, "html.parser").get_text(separator=" ").strip()
        summary_text = re.sub(r"\s+", " ", summary_text)
        summary_text = re.sub(r"https?://\S+", "", summary_text)
        summary_text = re.sub(r"ìì„¸íˆ ë³´ê¸°", "", summary_text, flags=re.IGNORECASE)

        phrases_to_remove = [
            r"ì´ ìƒˆë¡œìš´ ê¸°ëŠ¥ì€.*?ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\.",
            r"ìì„¸í•œ ë‚´ìš©ì€.*?ì°¸ê³ í•˜ì„¸ìš”\.",
            r"ê³ ê°ì€.*?ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\.",
        ]
        for phrase in phrases_to_remove:
            summary_text = re.sub(phrase, "", summary_text)

        return summary_text.strip()
    except Exception as e:
        print(f"âŒ AWS ìš”ì•½ í´ë Œì§• ì˜¤ë¥˜: {e}")
        return summary_html


def clean_gcp_summary(summary_html):
    try:
        summary_text = BeautifulSoup(summary_html, "html.parser").get_text(separator=" ").strip()
        summary_text = re.sub(r"\s+", " ", summary_text)
        summary_text = re.sub(r"https?://\S+", "", summary_text)
        summary_text = re.sub(r"ìì„¸íˆ ë³´ê¸°|Learn more", "", summary_text, flags=re.IGNORECASE)
        return summary_text.strip()
    except Exception as e:
        print(f"âŒ GCP ìš”ì•½ í´ë Œì§• ì˜¤ë¥˜: {e}")
        return summary_html

# ---------- ë°°ë„ˆ Base64 ----------
def get_banner_base64():
    banner_path = "banner.png"  # ë°°ë„ˆ ì´ë¯¸ì§€ íŒŒì¼ëª…
    with open(banner_path, "rb") as f:
        img_base64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:image/png;base64,{img_base64}"

# ---------- í°íŠ¸ ìë™ ì„¤ì • ----------
def set_korean_font():
    system = platform.system()
    if system == "Windows":
        plt.rc('font', family='Malgun Gothic')
    elif system == "Darwin":
        plt.rc('font', family='Apple SD Gothic Neo')
    else:
        plt.rc('font', family='NanumGothic')
    plt.rcParams['axes.unicode_minus'] = False

# ---------- Azure í¬ë¡¤ëŸ¬ ----------
def crawl_azure_updates():
    global all_data, stop_scraping
    print("ğŸš€ [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        try:
            page = browser.new_page()
            base_url = "https://azure.microsoft.com/en-us/updates?filters=%5B%22Launched%22%5D"
            try:
                page.goto(base_url, timeout=10000)
            except Exception as e:
                print(f"âŒ [Azure] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
                return

            page.wait_for_timeout(3000)

            pagination_items = page.query_selector_all("ul#pagination li")
            last_page_num = 1
            for item in pagination_items:
                text = item.inner_text().strip()
                if text.isdigit():
                    last_page_num = max(last_page_num, int(text))

            current_page = 1
            while current_page <= last_page_num:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(2)

                html = page.content()
                soup = BeautifulSoup(html, "html.parser")
                items = soup.select("li.ocr-faq-item.accordion_bg")

                for item in items:
                    try:
                        title_elem = item.select_one("h2.lead")
                        title = title_elem.get_text(strip=True) if title_elem else ""

                        content_elem = item.select_one("div.accordion-item.col-xl-8")
                        content = content_elem.get_text(strip=True).replace("\n", " ") if content_elem else ""

                        date_elem = (
                            item.select_one("div.updated_dates div.modified_date span")
                            or item.select_one("div.updated_dates div.created_date span")
                        )
                        date_text = date_elem.get_text(strip=True) if date_elem else ""
                        post_date = datetime.strptime(date_text, "%m/%d/%Y") if date_text else None

                        rss_link_elem = item.select_one('a[data-bi-cn="Accordion detail - Share RSS"]')
                        rss_link = rss_link_elem["href"] if rss_link_elem else ""

                        post_id = ""
                        if rss_link:
                            post_id = rss_link.split("/")[-1]

                        post_link = f"https://azure.microsoft.com/updates?id={post_id}" if post_id else ""

                        if post_date:
                            if start_of_week.date() <= post_date.date() <= end_of_week.date():
                                summary_en = summarize_text(content, sentence_count=2, max_length=300)
                                summary_ko = translate_to_korean(summary_en)

                                all_data.append({
                                    "source": "Azure",
                                    "title": title,
                                    "date": date_text,
                                    "content": content,
                                    "link": post_link,
                                    "summary": summary_ko
                                })
                            else:
                                stop_scraping = True
                                break

                    except Exception as e:
                        print(f"âŒ [Azure] {current_page}í˜ì´ì§€ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")

                if stop_scraping:
                    break

                next_page_num = current_page + 1
                if next_page_num > last_page_num:
                    break

                try:
                    next_li = page.query_selector(f'ul#pagination li:text-is("{next_page_num}")')
                    if next_li:
                        next_li.click()
                        page.wait_for_selector(f'ul#pagination li.active:text-is("{next_page_num}")', timeout=5000)
                        current_page += 1
                    else:
                        break
                except Exception:
                    break

        finally:
            browser.close()
    azure_count = len([d for d in all_data if d["source"] == "Azure"])
    print(f"ğŸ“„ [Azure] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - {azure_count}ê±´")

# ---------- AWS í¬ë¡¤ëŸ¬ ----------
def crawl_aws_updates():
    global all_data
    print("ğŸš€ [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
    feed_url = "https://aws.amazon.com/new/feed/"
    feed = feedparser.parse(feed_url)
    for entry in feed.entries:
        try:
            title = entry.title
            link = entry.link
            date_parsed = entry.published_parsed
            post_date = datetime(*date_parsed[:6])
            if start_of_week.date() <= post_date.date() <= end_of_week.date():
                cleaned_summary = clean_aws_summary(entry.summary)
                summary_en = summarize_text(cleaned_summary, sentence_count=1, max_length=150)
                summary_ko = translate_to_korean(summary_en)

                all_data.append({
                    "source": "AWS",
                    "title": title,
                    "date": post_date.strftime("%Y-%m-%d"),
                    "content": cleaned_summary,
                    "link": link,
                    "summary": summary_ko
                })
        except Exception as e:
            print(f"âŒ [AWS] RSS í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
    aws_count = len([d for d in all_data if d["source"] == "AWS"])
    print(f"ğŸ“„ [AWS] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - {aws_count}ê±´")

# ---------- GCP í¬ë¡¤ëŸ¬ ----------
def crawl_gcp_updates():
    global all_data
    print("ğŸš€ [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
    rss_url = "https://cloud.google.com/feeds/gcp-release-notes.xml"
    feed = feedparser.parse(rss_url)

    service_pattern = re.compile(r"([A-Z][a-zA-Z0-9\s\-\(\)]+)\s+(Feature|Announcement)\s+", re.MULTILINE)

    for entry in feed.entries:
        try:
            title = entry.title
            date_text = entry.get('published') or entry.get('updated')
            if not date_text:
                continue

            try:
                post_date = datetime.strptime(date_text, "%a, %d %b %Y %H:%M:%S %Z")
            except ValueError:
                post_date = datetime.strptime(date_text[:16], "%Y-%m-%dT%H:%M")

            if not (start_of_week.date() <= post_date.date() <= end_of_week.date()):
                continue

            raw_content = clean_gcp_summary(entry.summary)
            matches = list(service_pattern.finditer(raw_content))
            if not matches:
                continue

            for i, match in enumerate(matches):
                service_name = match.group(1).strip()
                category = match.group(2).strip()
                start_idx = match.end()
                end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(raw_content)
                content = raw_content[start_idx:end_idx].strip()

                if not content:
                    continue

                if "Security" in service_name or "CVE-" in content:
                    continue

                summary_en = summarize_long_text(content, max_chunks=3, max_length=150)
                summary_ko = translate_to_korean(summary_en)

                all_data.append({
                    "source": "GCP",
                    "title": f"{service_name} ({category})",
                    "date": post_date.strftime("%Y-%m-%d"),
                    "content": content,
                    "link": "https://cloud.google.com/release-notes",
                    "summary": summary_ko
                })

        except Exception as e:
            print(f"âŒ [GCP] RSS í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")

    gcp_count = len([d for d in all_data if d["source"] == "GCP"])
    print(f"ğŸ“„ [GCP] ì—…ë°ì´íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - {gcp_count}ê±´")

# ---------- íŒŒì´ì°¨íŠ¸ ìƒì„± ----------
def generate_summary_chart():
    set_korean_font()

    counts = {
        "Azure": len([d for d in all_data if d["source"] == "Azure"]),
        "AWS": len([d for d in all_data if d["source"] == "AWS"]),
        "GCP": len([d for d in all_data if d["source"] == "GCP"]),
    }

    total = sum(counts.values())

    def make_label(name, count):
        percent = (count / total) * 100 if total else 0
        return f"{name}\n{count}ê±´ ({percent:.1f}%)"

    labels = [make_label(name, count) for name, count in counts.items()]

    fig, ax = plt.subplots(figsize=(6, 6))
    wedges, texts = ax.pie(counts.values(), startangle=90)

    bbox_props = dict(boxstyle="round,pad=0.3", fc="white", ec="gray", lw=0.5)
    kw = dict(arrowprops=dict(arrowstyle="-"), bbox=bbox_props, zorder=0, va="center")

    for i, p in enumerate(wedges):
        ang = (p.theta2 - p.theta1) / 2. + p.theta1
        y = np.sin(np.deg2rad(ang))
        x = np.cos(np.deg2rad(ang))
        horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
        ax.annotate(labels[i], xy=(x, y), xytext=(1.2 * np.sign(x), 1.2 * y),
                    horizontalalignment=horizontalalignment, **kw)

    ax.set_title(f"ì´ë²ˆ ì£¼ Cloud ì—…ë°ì´íŠ¸ ìš”ì•½", fontsize=14)

    buf = BytesIO()
    plt.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode("utf-8")
    return f"data:image/png;base64,{img_base64}"

# ---------- íŒŒì¼ ì €ì¥ ----------
def save_files():
    os.makedirs("output", exist_ok=True)
    json_filename = f"output/cloud_updates_week_{today.strftime('%Y-%m-%d')}.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì—…ë°ì´íŠ¸ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {json_filename}")
    return json_filename

# ---------- ì´ë©”ì¼ ë°œì†¡ ----------
def send_email():
    if not all_data:
        print("â„¹ï¸ ì´ë²ˆ ì£¼ ì—…ë°ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë©”ì¼ ë°œì†¡í•˜ì§€ ì•ŠìŒ.")
        return

    msg = MIMEMultipart("alternative")
    msg["Subject"] = f"[Cloud Updates] ì´ë²ˆ ì£¼ ì†Œì‹ì§€ ({today.strftime('%Y-%m-%d')})"
    msg["From"] = SENDER
    msg["To"] = ", ".join(RECEIVERS)

    banner_base64 = get_banner_base64()
    chart_base64 = generate_summary_chart()

    html_content = f"""
    <h2>ğŸ“¢ í´ë¼ìš°ë“œ ì£¼ê°„ ì—…ë°ì´íŠ¸ ({today.strftime('%Y-%m-%d')})</h2>
    <img src="{banner_base64}" alt="ë°°ë„ˆ" style="width: 100%; max-width: 800px; margin-bottom: 20px;">
    <img src="{chart_base64}" alt="Cloud Update Summary Chart" style="max-width: 600px; margin-bottom: 20px;">
    """

    for provider in ["Azure", "AWS", "GCP"]:
        provider_items = [item for item in all_data if item["source"] == provider]
        if provider_items:
            icon = "ğŸ”·" if provider == "Azure" else ("ğŸŸ§" if provider == "AWS" else "ğŸŸ©")
            html_content += f"<h3>{icon} {provider} ì—…ë°ì´íŠ¸</h3><ul>"
            for item in provider_items:
                extra_note = f" (ë‚ ì§œ: {item['date']})" if provider == "GCP" else ""
                html_content += (
                    f"<li style='margin-bottom: 20px;'>"
                    f"<strong>{item['date']}</strong> - <strong>{item['title']}</strong><br>"
                    f"{item['summary']} <a href='{item['link']}' style='color: blue;'>ìì„¸íˆ ë³´ê¸°{extra_note}</a></li>"
                )
            html_content += "</ul>"

    msg.attach(MIMEText(html_content, "html"))
    send_with_retry(msg)

# ---------- ë©”ì¼ ì¬ì‹œë„ ----------
def send_with_retry(msg, retries=3):
    for attempt in range(retries):
        try:
            with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
                server.starttls()
                server.login(SENDER, SENDER_PASSWORD)
                server.sendmail(SENDER, RECEIVERS, msg.as_string())
            print("âœ… ë‰´ìŠ¤ë ˆí„° ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âŒ ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}): {e}")
            time.sleep(3)
    print("â—ï¸ ëª¨ë“  ë©”ì¼ ë°œì†¡ ì¬ì‹œë„ ì‹¤íŒ¨")

# ---------- ì‹¤í–‰ ----------
if __name__ == "__main__":
    crawl_azure_updates()
    crawl_gcp_updates()
    crawl_aws_updates()
    save_files()
    send_email()
==============
ì—ëŸ¬ì™€ ì½”ë“œë¥¼ ì²¨ë¶€í•©ë‹ˆë‹¤. í•´ê²°í•´ì£¼ì„¸ìš”.
