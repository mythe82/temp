(bitnet-cpp) malee@cstvm:~/BitNet$ python run_inference.py -m models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
warning: not compiled with GPU offload support, --gpu-layers option will be ignored
warning: see main README.md for information on enabling GPU BLAS support
build: 3955 (a8ac7072) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
main: llama backend init
main: load the model and apply lora adapter, if any
gguf_init_from_file: failed to open 'models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf': 'No such file or directory'
llama_model_load: error loading model: llama_model_loader: failed to load model from models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf

llama_load_model_from_file: failed to load model
common_init_from_params: failed to load model 'models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf'
main: error: unable to load model
Error occurred while running command: Command '['build/bin/llama-cli', '-m', 'models/Falcon3-7B-Instruct-1.58bit/ggml-model-i2_s.gguf', '-n', '128', '-t', '2', '-p', 'You are a helpful assistant', '-ngl', '0', '-c', '2048', '--temp', '0.8', '-b', '1', '-cnv']' returned non-zero exit status 1.
